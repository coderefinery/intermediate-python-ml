# Advanced Data Transformation in Pandas (15 mins)
## Lecture Materials with Exercises

### Introduction (1 minute)

**Slide 1: Beyond Basic Transformations**
- Data rarely comes in the exact format needed for analysis
- Advanced transformations help reshape and restructure data
- Element-wise operations let you apply custom logic to data
- Time and text data require specialized handling
- These techniques help you prepare data for visualization and modeling

**Talking Points:**
- "We've covered many fundamental data manipulation techniques, but real-world analysis often requires more advanced transformations."
- "These techniques let you apply custom logic to your data, work with specialized data types, and reshape data structures."
- "Mastering these transformations gives you the flexibility to prepare data for any type of analysis or visualization."
- "Today's techniques represent some of the most powerful aspects of Pandas for data scientists and analysts."

---

### 1. apply() and map() Functions (5 minutes)

**Slide 2: Element-wise Transformations**

| Function | Applied To | Purpose | Return Type |
|----------|------------|---------|-------------|
| `map()` | Series | Transform each value | Series |
| `apply()` | Series/DataFrame | Apply function to elements, rows, or columns | Series/DataFrame |
| `applymap()` | DataFrame | Apply function to each element | DataFrame |
| `pipe()` | DataFrame | Apply function to entire object | Varies |

**Code Example 1: Series Transformations with map() and apply()**
```python
import pandas as pd
import numpy as np

# Create a sample Series
s = pd.Series([1, 2, 3, 4, 5])
print("Original Series:")
print(s)

# 1. Using map() with a dictionary
grade_map = {1: 'F', 2: 'D', 3: 'C', 4: 'B', 5: 'A'}
grades = s.map(grade_map)
print("\n1. Using map() with a dictionary:")
print(grades)

# 2. Using map() with a function
squared = s.map(lambda x: x**2)
print("\n2. Using map() with a function:")
print(squared)

# 3. Using apply() with a function
cubed = s.apply(lambda x: x**3)
print("\n3. Using apply() with a function on a Series:")
print(cubed)

# 4. When values aren't in the mapping
s2 = pd.Series([1, 2, 6, 4, 9])
partial_map = s2.map(grade_map)
print("\n4. map() with missing dictionary keys:")
print(partial_map)  # Returns NaN for 6 and 9

# 5. Apply with a more complex function
def categorize(x):
    if x < 3:
        return 'Low'
    elif x < 5:
        return 'Medium'
    else:
        return 'High'
        
categories = s.apply(categorize)
print("\n5. apply() with a more complex function:")
print(categories)
```

**Code Example 2: DataFrame Transformations with apply()**
```python
# Create a sample DataFrame
df = pd.DataFrame({
    'A': [1, 2, 3, 4, 5],
    'B': [10, 20, 30, 40, 50],
    'C': [100, 200, 300, 400, 500]
})
print("Original DataFrame:")
print(df)

# 1. Apply to each column (default)
column_sums = df.apply(sum)
print("\n1. Column sums using apply():")
print(column_sums)

# 2. Apply to each row
row_sums = df.apply(sum, axis=1)
print("\n2. Row sums using apply(axis=1):")
print(row_sums)

# 3. Apply with a custom function
def range_calc(x):
    return x.max() - x.min()
    
ranges = df.apply(range_calc)
print("\n3. Range of each column:")
print(ranges)

# 4. Apply with a function that returns multiple values
def stats(x):
    return pd.Series([x.min(), x.max(), x.mean()], index=['min', 'max', 'mean'])
    
multi_stats = df.apply(stats)
print("\n4. Multiple statistics using apply():")
print(multi_stats)

# 5. Using applymap() for element-wise operations
formatted = df.applymap(lambda x: f"${x:,.2f}")
print("\n5. Format each value using applymap():")
print(formatted)
```

**Talking Points:**
- "The `map()` function is exclusively for Series and transforms each value according to a mapping or function."
- "Use `map()` with a dictionary when you have specific value replacements."
- "The `apply()` function is more versatile and works on both Series and DataFrames."
- "For DataFrames, `apply()` operates on entire rows or columns by default - use the `axis` parameter to control this."
- "When `apply()` returns a Series or DataFrame from each call, Pandas intelligently combines the results."
- "Use `applymap()` when you want to apply a function to every individual element in a DataFrame."
- "These functions are powerful because they let you apply any Python function to your data, from simple transformations to complex business logic."

**Exercise 1: apply() and map() Practice (1 minute)**

Have students execute:
```python
# Sample employee data
employees = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],
    'Salary': [75000, 86000, 120000, 65000, 95000],
    'Department': ['HR', 'Engineering', 'Management', 'Marketing', 'Engineering'],
    'Years_Experience': [3, 5, 15, 2, 8]
})
print("Employee DataFrame:")
print(employees)

# Tasks:
# 1. Use map() to categorize departments
dept_category = {'HR': 'Support', 'Engineering': 'Technical', 
                'Management': 'Leadership', 'Marketing': 'Support'}
employees['Dept_Category'] = employees['Department'].map(dept_category)
print("\n1. With department categories:")
print(employees)

# 2. Use apply() to calculate a bonus based on salary and experience
def calculate_bonus(row):
    base_percent = 0.05  # 5% base bonus
    experience_factor = min(row['Years_Experience'] * 0.01, 0.15)  # Up to 15% for experience
    return int(row['Salary'] * (base_percent + experience_factor))

employees['Bonus'] = employees.apply(calculate_bonus, axis=1)
print("\n2. With calculated bonuses:")
print(employees)

# 3. Apply a custom format to salaries and bonuses
employees['Formatted_Salary'] = employees['Salary'].apply(lambda x: f"${x:,}")
employees['Formatted_Bonus'] = employees['Bonus'].apply(lambda x: f"${x:,}")
print("\n3. With formatted amounts:")
print(employees[['Name', 'Formatted_Salary', 'Formatted_Bonus']])
```

**Expected Learning Outcome:** Students should understand the differences between map() and apply(), and how to use them for various transformation tasks on both Series and DataFrames.

---

### 2. String and Datetime Operations (4 minutes)

**Slide 3: Specialized Data Type Operations**

| String Operations | Datetime Operations |
|-------------------|---------------------|
| `str.lower()`, `str.upper()` | `dt.year`, `dt.month`, `dt.day` |
| `str.strip()`, `str.replace()` | `dt.hour`, `dt.minute`, `dt.second` |
| `str.contains()`, `str.startswith()` | `dt.dayofweek`, `dt.dayofyear`, `dt.quarter` |
| `str.split()`, `str.extract()` | `dt.strftime()`, `dt.tz_convert()` |
| `str.len()`, `str.cat()` | `dt.to_period()`, `dt.to_timestamp()` |

**Code Example 3: String Operations**
```python
# Create a DataFrame with string data
text_data = pd.DataFrame({
    'Name': ['  Alice Smith', 'BOB JOHNSON', 'charlie brown', 'David Miller, Jr.'],
    'Email': ['alice.smith@example.com', 'bob.j@company.co.uk', 'charlie@personal.net', 'david.m@work-email.org'],
    'Phone': ['(555) 123-4567', '555.987.6543', '(123) 876-5432', '987-654-3210'],
    'Tags': ['finance, reports', 'engineering, development', 'marketing, social media', 'design, UI/UX']
})
print("Text DataFrame:")
print(text_data)

# 1. Case transformations
text_data['Name_Lower'] = text_data['Name'].str.lower()
text_data['Name_Upper'] = text_data['Name'].str.upper()
text_data['Name_Title'] = text_data['Name'].str.title()
print("\n1. Case transformations:")
print(text_data[['Name', 'Name_Lower', 'Name_Upper', 'Name_Title']])

# 2. Cleaning and standardizing
text_data['Name_Clean'] = text_data['Name'].str.strip()
standardized_phone = text_data['Phone'].str.replace('[^\d]', '', regex=True)
print("\n2a. Cleaned names:")
print(text_data[['Name', 'Name_Clean']])
print("\n2b. Standardized phone numbers:")
print(pd.DataFrame({'Original': text_data['Phone'], 'Standardized': standardized_phone}))

# 3. Extraction
# Extract domain from email
domains = text_data['Email'].str.extract(r'@(.+)$')
print("\n3a. Extracted email domains:")
print(domains)

# Split tags into lists
split_tags = text_data['Tags'].str.split(', ')
print("\n3b. Split tags:")
print(split_tags)

# 4. Filtering with string methods
finance_related = text_data[text_data['Tags'].str.contains('finance')]
print("\n4. Finance-related rows:")
print(finance_related)
```

**Code Example 4: Datetime Operations**
```python
# Create a DataFrame with datetime data
date_data = pd.DataFrame({
    'Date': pd.date_range(start='2023-01-01', periods=6, freq='M'),
    'Event': ['New Year', 'Valentine', 'Spring Break', 'Conference', 'Product Launch', 'Summer Sale'],
    'Value': [1000, 1500, 2000, 2500, 3000, 3500]
})
print("Date DataFrame:")
print(date_data)

# 1. Extracting datetime components
date_data['Year'] = date_data['Date'].dt.year
date_data['Month'] = date_data['Date'].dt.month
date_data['Day'] = date_data['Date'].dt.day
date_data['Weekday'] = date_data['Date'].dt.day_name()
date_data['Quarter'] = date_data['Date'].dt.quarter
print("\n1. Extracted datetime components:")
print(date_data)

# 2. Formatting dates
date_data['Formatted_Date'] = date_data['Date'].dt.strftime('%B %d, %Y')
print("\n2. Formatted dates:")
print(date_data[['Date', 'Formatted_Date']])

# 3. Date calculations
today = pd.Timestamp('2023-07-15')
date_data['Days_From_Today'] = (today - date_data['Date']).dt.days
print("\n3. Days from July 15, 2023:")
print(date_data[['Date', 'Days_From_Today']])

# 4. Filtering by date components
q1_data = date_data[date_data['Date'].dt.quarter == 1]
print("\n4. Q1 data only:")
print(q1_data)

# 5. Resampling time series data
monthly_data = pd.Series(
    np.random.randint(100, 200, 365),
    index=pd.date_range(start='2023-01-01', periods=365)
)
print("\n5a. Daily time series (first 5 days):")
print(monthly_data.head())

# Resample to monthly
monthly_resampled = monthly_data.resample('M').mean()
print("\n5b. Resampled to monthly means:")
print(monthly_resampled)
```

**Talking Points:**
- "String and datetime operations are specialized tools for working with text and date data."
- "The `str` accessor provides methods that operate specifically on text in Series."
- "String operations are vectorized, making them much faster than iterating with loops."
- "The `dt` accessor unlocks datetime properties and methods for Series containing dates."
- "Working with datetime objects instead of strings makes time-based analysis much easier."
- "These methods handle many common data cleaning and feature extraction tasks."
- "They're essential for preparing time series data and text data for analysis."

**Exercise 2: String and Datetime Practice (1 minute)**

Have students execute:
```python
# Sample customer data with text and dates
customers = pd.DataFrame({
    'Customer_ID': ['CUS-001', 'CUS-002', 'CUS-003', 'CUS-004', 'CUS-005'],
    'Name': ['JOHN SMITH', 'mary johnson', '  Bob Williams', 'Sarah Brown  ', 'james MILLER'],
    'SignUp_Date': ['2022-03-15', '2022-01-07', '2023-05-20', '2021-11-30', '2023-02-14'],
    'Last_Purchase': ['2023-06-20', '2023-05-15', '2023-06-05', '2023-04-10', '2023-06-25']
})

# Convert date strings to datetime
customers['SignUp_Date'] = pd.to_datetime(customers['SignUp_Date'])
customers['Last_Purchase'] = pd.to_datetime(customers['Last_Purchase'])
print("Customer DataFrame:")
print(customers)

# Tasks:
# 1. Standardize customer names (title case, trimmed)
customers['Name_Standardized'] = customers['Name'].str.strip().str.title()
print("\n1. Standardized names:")
print(customers[['Name', 'Name_Standardized']])

# 2. Extract the numeric portion of the Customer_ID
customers['Customer_Number'] = customers['Customer_ID'].str.extract(r'(\d+)')
print("\n2. Extracted customer numbers:")
print(customers[['Customer_ID', 'Customer_Number']])

# 3. Calculate days since sign-up and days since last purchase
today = pd.Timestamp('2023-07-01')
customers['Days_Since_SignUp'] = (today - customers['SignUp_Date']).dt.days
customers['Days_Since_Purchase'] = (today - customers['Last_Purchase']).dt.days
print("\n3. Time-based metrics:")
print(customers[['Name_Standardized', 'Days_Since_SignUp', 'Days_Since_Purchase']])

# 4. Add a column for the month and year of sign-up
customers['SignUp_Month_Year'] = customers['SignUp_Date'].dt.strftime('%B %Y')
print("\n4. Sign-up month and year:")
print(customers[['Name_Standardized', 'SignUp_Date', 'SignUp_Month_Year']])
```

**Expected Learning Outcome:** Students should understand how to use the str and dt accessors to manipulate text and datetime data, and how these operations streamline common data preparation tasks.

---

### 3. Pivot Tables and Reshaping Operations (4 minutes)

**Slide 4: Reshaping Data Structures**

| Function | Purpose | Common Parameters |
|----------|---------|-------------------|
| `pivot_table()` | Create spreadsheet-like pivot table | `values`, `index`, `columns`, `aggfunc` |
| `pivot()` | Simple pivot (no aggregation) | `index`, `columns`, `values` |
| `melt()` | "Unpivot" from wide to long format | `id_vars`, `value_vars`, `var_name`, `value_name` |
| `stack()` | Pivot columns to index (wide to long) | `level`, `dropna` |
| `unstack()` | Pivot index to columns (long to wide) | `level`, `fill_value` |

**Code Example 5: Pivot Tables and Pivoting**
```python
# Create a sample sales DataFrame
sales_data = pd.DataFrame({
    'Date': pd.date_range(start='2023-01-01', periods=12),
    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
    'Region': ['East', 'East', 'West', 'West', 'North', 'North', 
               'South', 'South', 'East', 'West', 'North', 'South'],
    'Units': [10, 15, 12, 18, 9, 13, 14, 17, 11, 16, 8, 19],
    'Price': [100, 150, 100, 150, 100, 150, 100, 150, 100, 150, 100, 150]
})
sales_data['Revenue'] = sales_data['Units'] * sales_data['Price']
sales_data['Month'] = sales_data['Date'].dt.strftime('%B')
print("Sales DataFrame:")
print(sales_data)

# 1. Basic pivot table - sum of revenue by region and product
pivot1 = pd.pivot_table(
    sales_data,
    values='Revenue',
    index='Region',
    columns='Product'
)
print("\n1. Pivot table of revenue by region and product:")
print(pivot1)

# 2. Multiple values and aggregations
pivot2 = pd.pivot_table(
    sales_data,
    values=['Revenue', 'Units'],
    index='Region',
    columns='Month',
    aggfunc={'Revenue': 'sum', 'Units': 'mean'}
)
print("\n2. Pivot with multiple values and aggregations:")
print(pivot2)

# 3. Pivot with different aggregation functions
pivot3 = pd.pivot_table(
    sales_data,
    values='Revenue',
    index='Region',
    columns='Product',
    aggfunc=['sum', 'mean', 'count']
)
print("\n3. Pivot with multiple aggregation functions:")
print(pivot3)

# 4. Adding totals with margins
pivot4 = pd.pivot_table(
    sales_data,
    values='Revenue',
    index='Region',
    columns='Product',
    aggfunc='sum',
    margins=True,
    margins_name='Total'
)
print("\n4. Pivot table with totals:")
print(pivot4)
```

**Code Example 6: Melting and Reshaping**
```python
# Create a wide-format DataFrame (e.g., survey responses)
wide_data = pd.DataFrame({
    'Respondent': ['R001', 'R002', 'R003', 'R004', 'R005'],
    'Age': [25, 32, 45, 28, 51],
    'Q1_Score': [5, 4, 3, 4, 5],
    'Q2_Score': [4, 4, 5, 3, 2],
    'Q3_Score': [3, 5, 4, 4, 3]
})
print("Wide format data:")
print(wide_data)

# 1. Melt to long format
long_data = pd.melt(
    wide_data,
    id_vars=['Respondent', 'Age'],
    value_vars=['Q1_Score', 'Q2_Score', 'Q3_Score'],
    var_name='Question',
    value_name='Score'
)
print("\n1. Melted to long format:")
print(long_data)

# 2. Clean up Question column by removing '_Score'
long_data['Question'] = long_data['Question'].str.replace('_Score', '')
print("\n2. With cleaned question names:")
print(long_data)

# 3. Pivot back to wide format
wide_again = long_data.pivot(
    index='Respondent',
    columns='Question',
    values='Score'
)
print("\n3. Pivoted back to wide format:")
print(wide_again)

# 4. Stack and unstack operations
# First, set multiple indices
multi_index = sales_data.set_index(['Region', 'Product'])
print("\n4a. Multi-index DataFrame:")
print(multi_index.head())

# Stack to make columns into index levels
stacked = multi_index.stack()
print("\n4b. Stacked DataFrame:")
print(stacked.head(10))

# Unstack to make index levels into columns
unstacked = multi_index.unstack('Product')
print("\n4c. Unstacked DataFrame:")
print(unstacked.head())
```

**Talking Points:**
- "Reshaping operations transform data between wide and long formats, each useful for different types of analysis."
- "Pivot tables are similar to those in Excel, aggregating data along two dimensions."
- "`pivot_table()` is extremely versatile, handling multiple values and aggregation functions."
- "`melt()` is the opposite of pivoting, converting wide data to long format."
- "Long format is often better for statistical analysis and visualization with tools like Seaborn."
- "Wide format is usually more readable and works well for reporting."
- "`stack()` and `unstack()` provide more control over reshaping with multi-level indices."
- "These reshaping operations are essential for preparing data for different types of analysis and visualization."

**Exercise 3: Pivot Tables and Reshaping (1 minute)**

Have students execute:
```python
# Create a dataset of student scores across subjects and terms
scores = pd.DataFrame({
    'Student': ['Alice', 'Alice', 'Alice', 'Bob', 'Bob', 'Bob', 
                'Charlie', 'Charlie', 'Charlie'],
    'Subject': ['Math', 'Science', 'English', 'Math', 'Science', 'English',
                'Math', 'Science', 'English'],
    'Term': ['Fall', 'Fall', 'Fall', 'Spring', 'Spring', 'Spring',
             'Fall', 'Spring', 'Fall'],
    'Score': [85, 92, 78, 88, 76, 94, 90, 85, 92]
})
print("Student scores:")
print(scores)

# Tasks:
# 1. Create a pivot table of average scores by student and subject
student_subject = pd.pivot_table(
    scores,
    values='Score',
    index='Student',
    columns='Subject',
    aggfunc='mean'
)
print("\n1. Average scores by student and subject:")
print(student_subject)

# 2. Create a pivot table showing scores by subject and term with count and average
subject_term = pd.pivot_table(
    scores,
    values='Score',
    index='Subject',
    columns='Term',
    aggfunc=['mean', 'count']
)
print("\n2. Scores by subject and term (with mean and count):")
print(subject_term)

# 3. Reshape the original data to wide format
wide_scores = scores.pivot(
    index='Student',
    columns=['Subject', 'Term'],
    values='Score'
)
print("\n3. Scores in wide format:")
print(wide_scores)

# 4. Melt the wide format back to long format
melted = pd.melt(
    wide_scores.reset_index(),
    id_vars='Student',
    var_name=['Subject', 'Term'],
    value_name='Score'
)
print("\n4. Back to long format with melt():")
print(melted.head(9))
```

**Expected Learning Outcome:** Students should understand how to reshape data between wide and long formats using pivot_table(), pivot(), and melt(), and how these transformations prepare data for different types of analysis.

---

### Takeaway Message (1 minute)

**Slide 5: Key Takeaways for Advanced Data Transformation**

1. **Element-wise Operations:** `map()` and `apply()` let you implement custom logic for transforming data
2. **Specialized Accessors:** `str` and `dt` provide powerful methods for text and date manipulation
3. **Reshaping Tools:** Pivot tables and melting operations transform data between different formats
4. **Analysis Preparation:** These techniques prepare data for visualization, statistical analysis, and machine learning
5. **Flexibility:** Pandas' transformation capabilities let you handle virtually any data manipulation requirement

**Closing Remarks:**
"The advanced transformation techniques we've covered today complete your Pandas toolkit for data manipulation. With `apply()` and `map()`, you can implement custom transformations. String and datetime accessors give you specialized tools for text and time data. And reshaping operations let you structure your data in whatever format best suits your analysis. These capabilities, combined with all the other Pandas features we've learned, give you the power to handle virtually any data wrangling challenge. As you continue to work with Pandas, you'll find yourself combining these techniques to solve increasingly complex data manipulation problems."
