# Summary Statistics & Aggregations in Pandas (20 mins)
## Lecture Materials with Exercises

### Introduction (2 minutes)

**Slide 1: From Raw Data to Insights**
- Data collection and cleaning are just the beginning
- The real value comes from extracting meaningful insights
- Summary statistics reduce complex data into understandable metrics
- Aggregations allow us to analyze data at different levels of granularity
- These operations form the foundation of data analysis

**Talking Points:**
- "We've learned how to load, clean, and combine data in Pandas. Now we'll explore how to extract insights from that data."
- "Summary statistics and aggregations are how we transform raw data into actionable information."
- "These techniques allow us to answer questions like 'What's the average customer spending?' or 'How do sales compare across different regions?'"
- "The methods we'll learn today connect directly to statistical concepts but are implemented with Pandas' intuitive syntax."

---

### 1. Basic Descriptive Statistics (5 minutes)

**Slide 2: Built-in Statistical Methods**

| Method | Purpose | Returns |
|--------|---------|---------|
| `describe()` | Comprehensive summary | DataFrame with count, mean, std, min, quartiles, max |
| `mean()`, `median()` | Central tendency | Series or scalar |
| `min()`, `max()` | Range boundaries | Series or scalar |
| `std()`, `var()` | Dispersion/spread | Series or scalar |
| `quantile([0.25, 0.75])` | Distribution points | Series or DataFrame |
| `count()` | Non-null count | Series or scalar |
| `sum()` | Sum of values | Series or scalar |

**Code Example 1: Basic Statistical Methods**
```python
import pandas as pd
import numpy as np

# Create a sample DataFrame
np.random.seed(42)  # For reproducibility
data = {
    'Age': np.random.randint(18, 65, 12),
    'Income': np.random.normal(60000, 15000, 12).astype(int),
    'Years_Experience': np.random.randint(0, 30, 12),
    'Department': np.random.choice(['Sales', 'Engineering', 'Marketing', 'HR'], 12),
    'Performance': np.random.uniform(1, 5, 12).round(1)
}
df = pd.DataFrame(data)

# Add some missing values for realism
df.loc[3, 'Years_Experience'] = None
df.loc[7, 'Performance'] = None

print("Sample Employee DataFrame:")
print(df)

# 1. Comprehensive statistical summary
print("\n1. Comprehensive statistical summary with describe():")
print(df.describe())

# 2. Include all columns (even non-numeric)
print("\n2. Describing all columns (including non-numeric):")
print(df.describe(include='all'))

# 3. Basic statistical methods
print("\n3. Basic statistical methods:")
print(f"Mean age: {df['Age'].mean():.1f} years")
print(f"Median income: ${df['Income'].median():,}")
print(f"Minimum years of experience: {df['Years_Experience'].min()}")
print(f"Maximum performance rating: {df['Performance'].max():.1f}")
print(f"Standard deviation of income: ${df['Income'].std():.2f}")

# 4. Multiple statistics at once
print("\n4. Multiple statistics for Income:")
print(df['Income'].agg(['min', 'max', 'mean', 'median', 'std']))

# 5. Quantiles/percentiles
print("\n5. Income quartiles:")
print(df['Income'].quantile([0, 0.25, 0.5, 0.75, 1.0]))
```

**Code Example 2: Statistical Methods with GroupBy**
```python
# 6. Statistics by group
print("\n6. Mean values by department:")
print(df.groupby('Department').mean())

# 7. Multiple statistics by group
print("\n7. Multiple statistics for Performance by department:")
dept_perf = df.groupby('Department')['Performance'].agg(['count', 'min', 'max', 'mean'])
print(dept_perf)

# 8. Visualizing statistics (with just code preview)
print("\n8. Code for visualizing statistics (not executed here):")
print("import matplotlib.pyplot as plt")
print("df.boxplot(column='Income', by='Department', figsize=(10, 6))")
print("plt.title('Income Distribution by Department')")
print("plt.suptitle('')  # Remove default title")
print("plt.tight_layout()")
print("plt.show()")
```

**Talking Points:**
- "The `describe()` method gives you a quick overview of your data's distribution."
- "Notice how Pandas handles missing values in statistical calculations - they're automatically excluded."
- "These methods work column-wise by default, which makes sense for most statistical analyses."
- "You can compute multiple statistics at once with the `agg()` method, which we'll explore further."
- "Statistical methods can be combined with grouping to compare metrics across categories."
- "Remember that these methods only make sense for numerical data - Pandas will ignore or handle non-numeric data differently."

**Exercise 1: Basic Statistics (1 minute)**

Have students execute:
```python
# Create a dataset of student scores
student_data = {
    'Student_ID': range(1, 11),
    'Math': [85, 90, 72, 95, 83, 78, 92, 86, 79, 88],
    'Science': [92, 85, 76, 94, 88, 84, 90, 81, 74, 89],
    'English': [78, 92, 88, 96, 82, 79, 91, 84, 76, 93],
    'Class': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']
}
students = pd.DataFrame(student_data)

# Tasks:
# 1. Generate a comprehensive statistical summary of all numeric columns
print("1. Statistical summary of student scores:")
print(students.describe())

# 2. Calculate the average score for each subject
subject_means = students[['Math', 'Science', 'English']].mean()
print("\n2. Average scores by subject:")
print(subject_means)

# 3. Find the highest and lowest scoring student in each subject
highest_math = students.loc[students['Math'].idxmax()]
lowest_english = students.loc[students['English'].idxmin()]
print("\n3. Highest scoring student in Math:")
print(highest_math[['Student_ID', 'Math', 'Class']])
print("\nLowest scoring student in English:")
print(lowest_english[['Student_ID', 'English', 'Class']])

# 4. Compare average scores between classes
class_comparison = students.groupby('Class')[['Math', 'Science', 'English']].mean()
print("\n4. Average scores by class:")
print(class_comparison)
```

**Expected Learning Outcome:** Students should understand how to use Pandas' built-in statistical methods to analyze data both globally and by groups.

---

### 2. Custom Aggregation Functions (4 minutes)

**Slide 3: Beyond Built-in Statistics**

| Approach | Use Case | Method |
|----------|----------|--------|
| Built-in aggregations | Common statistics | `.agg(['mean', 'median'])` |
| Lambda functions | Simple custom calculations | `.agg(lambda x: x.max() - x.min())` |
| Named functions | Complex custom calculations | `.agg(custom_function)` |
| Dictionary approach | Different aggs per column | `.agg({'A': 'sum', 'B': 'mean'})` |
| Multiple aggregations | Comprehensive analysis | `.agg(['count', 'mean', custom_func])` |

**Code Example 3: Custom Aggregation Functions**
```python
# Continue with employee DataFrame
print("Original Employee DataFrame:")
print(df)

# 1. Simple custom aggregation with lambda
range_calc = df['Income'].agg(lambda x: x.max() - x.min())
print("\n1. Income range using lambda function:")
print(f"${range_calc:,}")

# 2. Define a custom function
def range_ratio(x):
    """Calculate the ratio of max to min value"""
    return x.max() / x.min()

print("\n2. Income max/min ratio using custom function:")
print(f"{df['Income'].agg(range_ratio):.2f}")

# 3. Multiple custom and built-in aggregations
print("\n3. Multiple aggregations on Age:")
age_stats = df['Age'].agg([
    'mean',
    'median',
    'std',
    ('range', lambda x: x.max() - x.min()),
    ('cv', lambda x: x.std() / x.mean())  # Coefficient of variation
])
print(age_stats)

# 4. Different aggregations for different columns
print("\n4. Different aggregations per column:")
mixed_aggs = df.agg({
    'Age': ['min', 'max', 'mean'],
    'Income': ['median', 'std'],
    'Years_Experience': 'sum',
    'Performance': ['mean', range_ratio]
})
print(mixed_aggs)
```

**Code Example 4: Custom Aggregations with GroupBy**
```python
# 5. Custom aggregations by group
print("\n5. Custom aggregations by department:")
dept_custom = df.groupby('Department').agg({
    'Age': ['mean', 'std'],
    'Income': ['median', ('range', lambda x: x.max() - x.min())],
    'Performance': ['mean', ('above_avg', lambda x: (x > 3.0).mean() * 100)]
})
print(dept_custom)

# 6. Flatten hierarchical column names
dept_custom.columns = ['_'.join(col).strip() for col in dept_custom.columns.values]
print("\n6. With flattened column names:")
print(dept_custom)
```

**Talking Points:**
- "Built-in aggregations cover many common needs, but custom functions let you calculate any metric you can imagine."
- "Lambda functions are perfect for simple calculations that don't need to be reused."
- "Named functions are better for complex calculations or ones you'll reuse multiple times."
- "The dictionary approach gives you fine-grained control over which aggregations to apply to each column."
- "Custom aggregations really shine when combined with grouping operations."
- "Note how the result has a hierarchical index - flattening these can make the results more manageable."
- "These techniques are powerful because they let you create domain-specific metrics tailored to your analysis needs."

**Exercise 2: Custom Aggregations (1 minute)**

Have students execute:
```python
# Continue with the student dataset
print("Student DataFrame:")
print(students)

# Tasks:
# 1. Calculate the range (max - min) for each subject
score_ranges = students.agg({
    'Math': lambda x: x.max() - x.min(),
    'Science': lambda x: x.max() - x.min(),
    'English': lambda x: x.max() - x.min()
})
print("\n1. Score range for each subject:")
print(score_ranges)

# 2. Define and apply a custom function to calculate the proportion of students scoring above 85
def high_achievers(scores):
    return (scores > 85).mean() * 100  # Percentage of scores above 85

high_performers = students[['Math', 'Science', 'English']].agg(high_achievers)
print("\n2. Percentage of high achievers (>85) in each subject:")
print(high_performers)

# 3. Calculate multiple statistics by class
class_stats = students.groupby('Class').agg({
    'Math': ['mean', 'max', high_achievers],
    'Science': ['mean', 'max', high_achievers],
    'English': ['mean', 'max', high_achievers]
})
print("\n3. Multiple statistics by class:")
print(class_stats)
```

**Expected Learning Outcome:** Students should understand how to create and apply custom aggregation functions, both simple lambda functions and more complex named functions, and how to apply different aggregations to different columns.

---

### 3. GroupBy Operations and the Split-Apply-Combine Pattern (7 minutes)

**Slide 4: The Split-Apply-Combine Workflow**

![Split-Apply-Combine Pattern](/api/placeholder/600/300)

| Phase | Description | Pandas Implementation |
|-------|-------------|----------------------|
| Split | Divide data into groups | `df.groupby('column')` |
| Apply | Perform operation on each group | `.mean()`, `.sum()`, `.agg()`, etc. |
| Combine | Bring results back together | Automatic in Pandas |

**Code Example 5: Basic GroupBy Operations**
```python
# Create a more comprehensive dataset
retail_data = {
    'Date': pd.date_range(start='2023-01-01', periods=20),
    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], 20),
    'Category': np.random.choice(['Electronics', 'Accessories'], 20),
    'Store': np.random.choice(['North', 'South', 'East', 'West'], 20),
    'Units': np.random.randint(1, 10, 20),
    'Unit_Price': np.random.choice([1200, 800, 300, 250], 20),
    'Discount': np.random.choice([0, 0.1, 0.2], 20)
}
sales = pd.DataFrame(retail_data)
sales['Revenue'] = sales['Units'] * sales['Unit_Price'] * (1 - sales['Discount'])

print("Retail Sales DataFrame:")
print(sales.head())

# 1. Basic groupby with single column
product_sales = sales.groupby('Product')['Units'].sum()
print("\n1. Total units sold by product:")
print(product_sales)

# 2. Groupby with multiple aggregations
product_summary = sales.groupby('Product').agg({
    'Units': 'sum',
    'Revenue': ['sum', 'mean'],
    'Discount': 'mean'
})
print("\n2. Product sales summary:")
print(product_summary)

# 3. Groupby with multiple columns
store_product = sales.groupby(['Store', 'Product'])['Revenue'].sum()
print("\n3. Revenue by store and product:")
print(store_product)

# 4. Unstack to reshape result
store_product_unstacked = store_product.unstack()
print("\n4. Revenue by store and product (unstacked):")
print(store_product_unstacked)

# 5. Groupby with date components
sales['Month'] = sales['Date'].dt.month
monthly_sales = sales.groupby('Month')['Revenue'].sum()
print("\n5. Total monthly revenue:")
print(monthly_sales)
```

**Code Example 6: Advanced GroupBy Operations**
```python
# 1. Filter groups based on a condition
high_volume_products = sales.groupby('Product').filter(lambda x: x['Units'].sum() > 30)
print("1. Products with more than 30 total units sold:")
print(high_volume_products['Product'].unique())

# 2. Transform to fill missing values with group mean
df_with_missing = sales.copy()
df_with_missing.loc[2, 'Unit_Price'] = None
df_with_missing.loc[5, 'Unit_Price'] = None
print("\n2a. DataFrame with missing values:")
print(df_with_missing[df_with_missing['Unit_Price'].isna()])

# Fill with product-specific means
filled_df = df_with_missing.copy()
filled_df['Unit_Price'] = df_with_missing.groupby('Product')['Unit_Price'].transform(
    lambda x: x.fillna(x.mean())
)
print("\n2b. After filling with product-specific means:")
print(filled_df[df_with_missing['Unit_Price'].isna()])

# 3. Apply to manipulate each group
def top_revenue_product(group):
    return group.nlargest(1, 'Revenue')

top_by_store = sales.groupby('Store').apply(top_revenue_product)
print("\n3. Highest revenue product in each store:")
print(top_by_store[['Store', 'Product', 'Revenue']])

# 4. Calculating group-specific ranks
sales['Revenue_Rank'] = sales.groupby('Store')['Revenue'].rank(ascending=False)
top_sellers = sales[sales['Revenue_Rank'] <= 2]
print("\n4. Top 2 sales by revenue in each store:")
print(top_sellers[['Store', 'Product', 'Revenue', 'Revenue_Rank']])
```

**Talking Points:**
- "The GroupBy operation is one of the most powerful features in Pandas, inspired by the 'split-apply-combine' pattern from R."
- "First, data is split into groups based on one or more columns."
- "Next, operations are applied to each group independently."
- "Finally, results are combined back into a coherent structure."
- "This pattern is incredibly flexible - you can group by any column(s) and apply virtually any operation."
- "The result of a groupby has a hierarchical index, which can be reshaped using `unstack()`."
- "Beyond simple aggregations, groupby supports filtering, transformation, and applying custom functions to groups."
- "The `transform()` method is particularly useful for filling missing values with group-specific statistics."
- "The `apply()` method allows for complex operations that operate on the entire group."
- "Groupby operations often form the backbone of business intelligence and reporting systems."

**Exercise 3: GroupBy Operations (2 minutes)**

Have students execute:
```python
# Create a dataset of sales across regions and product categories
sales_data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 
               'East', 'East', 'East', 'West', 'West', 'West'],
    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],
    'Quarter': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],
    'Sales': [100, 150, 120, 90, 110, 95, 115, 140, 130, 80, 120, 105],
    'Customers': [20, 25, 18, 15, 22, 16, 23, 28, 19, 14, 21, 17]
}
quarterly_sales = pd.DataFrame(sales_data)
print("Quarterly Sales Data:")
print(quarterly_sales)

# Tasks:
# 1. Calculate total sales by region
region_sales = quarterly_sales.groupby('Region')['Sales'].sum()
print("\n1. Total sales by region:")
print(region_sales)

# 2. Find average sales and customer count by product
product_avg = quarterly_sales.groupby('Product').agg({
    'Sales': 'mean',
    'Customers': 'mean'
})
print("\n2. Average sales and customers by product:")
print(product_avg)

# 3. Create a pivot table of Sales by Region and Product
sales_pivot = quarterly_sales.pivot_table(
    values='Sales', 
    index='Region', 
    columns='Product',
    aggfunc='sum'
)
print("\n3. Sales pivot table (Region × Product):")
print(sales_pivot)

# 4. Calculate the sales per customer by region and quarter
quarterly_sales['Sales_per_Customer'] = quarterly_sales['Sales'] / quarterly_sales['Customers']
region_quarter = quarterly_sales.groupby(['Region', 'Quarter'])[['Sales', 'Sales_per_Customer']].mean()
print("\n4. Sales metrics by region and quarter:")
print(region_quarter)
```

**Expected Learning Outcome:** Students should understand the split-apply-combine pattern and how to use groupby operations to perform aggregations, transformations, and custom operations on grouped data.

---

### Takeaway Message (2 minutes)

**Slide 5: Key Takeaways for Summary Statistics & Aggregations**

1. **Descriptive Statistics:** Pandas provides comprehensive methods for calculating summary statistics on your data
2. **Custom Aggregations:** When built-in statistics aren't enough, you can create custom aggregation functions
3. **GroupBy Power:** The split-apply-combine pattern enables sophisticated analyses across subsets of your data
4. **Multiple Dimensions:** You can group by multiple columns to create hierarchical analyses
5. **Reshaping Results:** Tools like unstack() and pivot_table() help transform grouped results into useful formats

**Slide 6: Data Analysis Workflow**

```
Raw Data → Cleaning → Merging → Grouping & Aggregation → Visualization → Insights
```

**Closing Remarks:**
"The statistical and aggregation capabilities we've explored today transform Pandas from a data manipulation tool into a powerful analysis platform. These methods allow you to extract meaningful insights and answer complex questions about your data. As you continue to work with Pandas, you'll find that the combination of flexible data structures, powerful indexing, and these aggregation techniques form a comprehensive toolkit for data analysis. In the next session, we'll explore advanced data transformation techniques that build on these aggregation concepts. Let's take a 5-minute break before continuing."